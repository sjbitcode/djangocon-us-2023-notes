## Postgres Performance: From Slow to Pro
### by Elizabeth Christensen

- [talk](https://2023.djangocon.us/talks/postgres-performance-from-slow-to-pro/)
- [@sqlliz@fosstodon.org](https://fosstodon.org/@sqlliz)
---

- Postgres is very stable, easy integration with Django
- DB performance goals
  - cost, speed, optimize
  - maximize what you're getting out of the db
- Roadmap
  - data flow
  - settings
  - memory tuning
  - CPU/IOPS (whats going on inside Postgres/how your app interacting with the db)
  - connection usage
  - query performance
  - query fixes
- if you're using a managed service (RDS, etc) you may have preconfigurations, but its good to know what they are
- Data flow
  - high level
    - web server
    - app layer
    - db layer
  - writing data goes through shared buffer into disk storage
  - reading data will be in shared buffers, so need to go to disk storage
- IOPS - every write will reach underlying disk
  - you want to maximize what's in shared buffers, the more stuff you read from disk, the slower
  - postgres stores when query comes in and if it read from cache or disk
    - check slides for query to check cache hit ratio
    - high 90s is good
  - each connection (client backend) uses its own memory (work mem), default 4MB (too small for prod use case)
    - spills to temp file
    - check how many files generated to see if work mem is too small
  - pgtune to check recommended settings for all this
- CPU and system load
  - `SELECT * FROM pg_stat_activity`
  - you can set timeout for long running processes
- IOPS
  - indicator of when you don't have enough working memory or shared buffers
  - i.e. postgres using disk too much for reads/writes
  - don't want to see high IOPS all the time
- Table bloat
  - postgres keeps old data around and cleans up after since multiple queries happening simultaneously
  - dead tuples
  - 50% is ok; postgres will auto-vacuum
- Posgtres versions
  - postgres 12 EOL, you should be running >=13 
- Connections
  - query to count number of connections, what queries are using connections
  - use connection pool to manage open connections
  - pgBouncer, there's more
- Query Hunting
  - looking for issues within application
    - app logs, postgres logs, queries
      - django logging, debug toolbar, django silk
  - what is fast and slow?
    - ~1ms is normal
    - bigger queries, 100-300ms
    - more than 5000ms (5 seconds) is bad!
  - postgres logs
    - third party, APM tools, pdbadger
    - you can tell postgres to log duration of operations
    - logging locks and waits -- this query is slow because something before locked it
    - pgstat statements
      - devs use this! turn on `pg_stat_statements`!
      - see slides slides (~29:00 mark) on
        - 10 longest running queries
        - what queries are running often
        - what queries by cpu usage
      - explain plan
        - how a scan is done, indexes used, cost
      - explain analyze
        - runs the query and gives you planning/execution time
    - postgres parses, generate fastest path, make query plan, execute + return data for each query
      - if you want to change plans, you should test on prod data
    - explain - scan types
      - sequential, bitmap heap, bitmap index, index
      - join terms - nested loops, hash join, merge join
    - auto explain - puts explain plans in postgres logs
      - do this carefully on test data, takes up a lot of space and memory for calculations!
    - query + operation enhancements
      - indexing
        - group blocks of data together so its easier to find
        - b-tree: general purpose
        - brin: timestamps or date ranges
        - gist: spatial data/full text search
        - gin: array or JSON
        - primary key
        - for web devs:
          - multi-column index
          - partial index - helps reduce size of index if you have a lot of nulls/unused records
        - indexes stored on disk and can take time to create
          - hypothetical indexing - ask postgres for explain plan on index that doesn't exist
        - don't create too many indexes! every write needs to update index
          - you can ask postgres what are unused indexes
      - data modelling
        - keep tables small
        - updating too much data at a time
        - login timestamps tied to contact info (don't have these in the same table!)
          - separating data that shouldn't be together
          - keep frequently updated data separate
          - table bloat: every update, pg keeps dead row around -> data amplification
      - n+1 queries
        - queries that spin off another query
        - one large query is more efficient for the db
        - `select_related`, `prefetch_related`, `iterator()` will let you cache results
- Last tips for performance
  - stop runaway queries, set timeout
  - use `pg_stat` statements, know what slowest queries are
  - add indexes for frequent queries
  - check cache hit ratio
  - tune memory/memory connections (use pgbouncer)
  - stay on top of your postgres versions!
